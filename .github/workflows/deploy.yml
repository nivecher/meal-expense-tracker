---
name: Deploy

on:
  workflow_run:
    workflows: [CI]
    types:
      - completed
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: Environment to deploy to
        required: true
        default: dev
        type: choice
        options:
          - dev
          - staging
          - prod
  repository_dispatch:
    types:
      - deploy-staging

# Concurrency control to prevent duplicate runs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.event.inputs.environment || 'dev' }}
  cancel-in-progress: false  # Don't cancel deployments in progress

env:
  APP_NAME: meal-expense-tracker
  PYTHON_VERSION: 3.13
  NODE_VERSION: 22
  PYTHONPATH: ${{ github.workspace }}
  FLASK_ENV: test
  TESTING: true

  # Deployment settings
  ENV: ${{ github.event.inputs.environment || 'dev' }}
  TF_ENV: ${{ github.event.inputs.environment || 'dev' }}
  AWS_REGION: us-east-1
  ECR_REGISTRY: ${{ vars.ECR_REGISTRY || '123456789012.dkr.ecr.us-east-1.amazonaws.com' }}
  # Cache settings
  CACHE_VERSION: v3

# Required permissions for the workflow
permissions:
  contents: read
  id-token: write
  actions: read
  checks: write
  statuses: write
  security-events: write
  packages: write  # Changed from read to write for GHCR push

jobs:
  # Check CI workflow status and get version from tag
  check-ci:
    name: Check CI Status & Get Version
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      ci_passed: >
        ${{ github.event_name == 'workflow_run' &&
        github.event.workflow_run.conclusion == 'success' ||
        github.event_name == 'workflow_dispatch' }}
      version: ${{ steps.get_version.outputs.version }}
      tag: ${{ steps.get_version.outputs.tag }}
    steps:
      - name: Check CI workflow status
        run: |
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            if [ "${{ github.event.workflow_run.conclusion }}" != "success" ]; then
              echo "CI workflow did not succeed, skipping deployment"
              exit 1
            fi
            echo "CI workflow succeeded, proceeding with deployment"
          else
            echo "Manual trigger via workflow_dispatch, proceeding with deployment"
          fi

      - name: Get version from latest tag
        id: get_version
        run: |
          git fetch --tags --force || true
          TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo "")
          if [ -n "$TAG" ]; then
            VERSION=$(echo "$TAG" | sed 's/^v//')
            echo "tag=${TAG}" >> $GITHUB_OUTPUT
            echo "version=${VERSION}" >> $GITHUB_OUTPUT
            echo "Found tag from git: ${TAG}, version: ${VERSION}"
          else
            echo "tag=dev" >> $GITHUB_OUTPUT
            echo "version=dev" >> $GITHUB_OUTPUT
            echo "No tag found, using dev"
          fi

  # Build and push Docker image
  build:
    name: Build and Push
    needs: [check-ci]
    if: >
      ${{ github.event_name != 'pull_request' &&
      needs.check-ci.outputs.ci_passed == 'true' }}
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'dev' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip
          cache-dependency-path: requirements*.txt

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y make

      - name: Create virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=raw,value=${{ needs.check-ci.outputs.version || 'dev' }},enable=${{ needs.check-ci.outputs.version != '' }}
            type=raw,value=latest,enable=${{ github.ref == 'refs/heads/main' && needs.check-ci.outputs.version != '' }}
            type=sha,format=short
          labels: |
            org.opencontainers.image.version=${{ needs.check-ci.outputs.version || 'dev' }}
            org.opencontainers.image.revision=${{ github.sha }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # Deploy to development environment
  deploy-dev:
    name: Deploy to Dev
    needs: [check-ci, build]
    if: >
      ${{ github.event_name != 'pull_request' &&
      needs.check-ci.outputs.ci_passed == 'true' &&
      (github.ref == 'refs/heads/main' ||
      github.event.inputs.environment == 'dev' ||
      github.event_name == 'workflow_run') }}
    environment: dev
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    outputs:
      version: ${{ needs.check-ci.outputs.version }}
      tag: ${{ needs.check-ci.outputs.tag }}
      deployment_url: ${{ steps.get_url.outputs.url }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-${{ github.run_id }}
          role-duration-seconds: 3600

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.5
          terraform_wrapper: false

      - name: Setup Terraform plugin cache
        run: |
          mkdir -p ${{ runner.temp }}/.terraform.d/plugin-cache
          echo "Terraform plugin cache directory created: ${{ runner.temp }}/.terraform.d/plugin-cache"

      - name: Generate backend.hcl files from templates
        env:
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          if [ -d "terraform" ]; then
            # Get AWS account ID
            AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            echo "Using AWS Account ID: $AWS_ACCOUNT_ID"
            echo "Using AWS Region: $AWS_REGION"
            # Generate backend.hcl files for each environment from templates
            for ENV in dev staging prod; do
              TEMPLATE_FILE="terraform/environments/${ENV}/backend.hcl.template"
              OUTPUT_FILE="terraform/environments/${ENV}/backend.hcl"
              if [ -f "$TEMPLATE_FILE" ]; then
                echo "Generating $OUTPUT_FILE from template..."
                # Replace ${AWS_ACCOUNT_ID} and ${AWS_REGION} placeholders
                sed -e "s/\${AWS_ACCOUNT_ID}/$AWS_ACCOUNT_ID/g" \
                    -e "s/\${AWS_REGION}/$AWS_REGION/g" \
                    "$TEMPLATE_FILE" > "$OUTPUT_FILE"
                echo "âœ… Generated $OUTPUT_FILE"
              else
                echo "âš ï¸  Template file $TEMPLATE_FILE not found, skipping..."
              fi
            done
          else
            echo "No terraform directory found, skipping backend.hcl generation"
          fi

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y make jq

      - name: Deploy Infrastructure (Terraform)
        env:
          TF_PLUGIN_CACHE_DIR: ${{ runner.temp }}/.terraform.d/plugin-cache
          TF_IN_AUTOMATION: 1
        run: |
          make TF_ENV=dev tf-init
          make TF_ENV=dev tf-plan
          make TF_ENV=dev tf-apply

      - name: Get ECR repository URI
        id: ecr_repo
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_REPO="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/meal-expense-tracker-${TF_ENV}-lambda"
          echo "repository=${ECR_REPO}" >> $GITHUB_OUTPUT
          echo "ECR Repository: ${ECR_REPO}"

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | \
            docker login --username AWS --password-stdin ${{ steps.ecr_repo.outputs.repository }}

      - name: Tag and push Docker image to ECR
        run: |
          IMAGE_TAG="${{ needs.check-ci.outputs.version || 'dev' }}"
          ECR_REPO="${{ steps.ecr_repo.outputs.repository }}"
          
          # Tag image with version and latest
          docker tag ghcr.io/${{ github.repository }}:${IMAGE_TAG} ${ECR_REPO}:${IMAGE_TAG}
          docker tag ghcr.io/${{ github.repository }}:${IMAGE_TAG} ${ECR_REPO}:latest
          
          # Push both tags
          docker push ${ECR_REPO}:${IMAGE_TAG}
          docker push ${ECR_REPO}:latest

      - name: Update Lambda function
        run: |
          FUNCTION_NAME="meal-expense-tracker-dev"
          ECR_REPO="${{ steps.ecr_repo.outputs.repository }}"
          
          aws lambda update-function-code \
            --function-name ${FUNCTION_NAME} \
            --image-uri ${ECR_REPO}:latest \
            --region ${{ env.AWS_REGION }}
          
          # Wait for Lambda to be ready
          aws lambda wait function-updated \
            --function-name ${FUNCTION_NAME} \
            --region ${{ env.AWS_REGION }}

      - name: Sync static files to S3
        env:
          ENVIRONMENT: dev
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          chmod +x scripts/sync_static_to_s3.sh
          ./scripts/sync_static_to_s3.sh || echo "Warning: Static file sync failed, but continuing..."

      - name: Get deployment URL
        id: get_url
        run: |
          # Try to get API Gateway URL from Terraform outputs or AWS
          API_URL=$(aws apigatewayv2 get-apis \
            --query "Items[?contains(Name, 'meal-expense-tracker-dev')].ApiEndpoint" \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          
          if [ -z "$API_URL" ]; then
            # Fallback: try CloudFront URL from Terraform
            API_URL=$(cd terraform && terraform output -raw api_url 2>/dev/null || echo "")
          fi
          
          if [ -z "$API_URL" ]; then
            API_URL="https://meals.dev.nivecher.com"
          fi
          
          echo "url=${API_URL}" >> $GITHUB_OUTPUT
          echo "Deployment URL: ${API_URL}"

      - name: Verify Deployment
        id: verify
        run: |
          DEPLOYMENT_URL="${{ steps.get_url.outputs.url }}"
          echo "Verifying deployment at: ${DEPLOYMENT_URL}"
          
          # Health check with exponential backoff
          MAX_ATTEMPTS=15
          for i in $(seq 1 $MAX_ATTEMPTS); do
            if curl -f -s "${DEPLOYMENT_URL}/health" > /dev/null; then
              echo "âœ… Health check passed on attempt ${i}"
              echo "deployment_healthy=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            WAIT_TIME=$((2 ** (i - 1)))
            if [ $WAIT_TIME -gt 60 ]; then
              WAIT_TIME=60
            fi
            echo "Attempt ${i}/${MAX_ATTEMPTS} failed, retrying in ${WAIT_TIME} seconds..."
            sleep $WAIT_TIME
          done
          
          echo "âŒ Health check failed after ${MAX_ATTEMPTS} attempts"
          echo "deployment_healthy=false" >> $GITHUB_OUTPUT
          exit 1

      - name: Get previous Lambda version
        if: steps.verify.outcome == 'failure'
        id: previous_version
        run: |
          FUNCTION_NAME="meal-expense-tracker-dev"
          # Get current function configuration to find previous version
          CURRENT_VERSION=$(aws lambda get-function \
            --function-name ${FUNCTION_NAME} \
            --query 'Configuration.Version' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          
          if [ -n "$CURRENT_VERSION" ] && [ "$CURRENT_VERSION" != "\$LATEST" ]; then
            # Get aliases to find previous version
            PREVIOUS_ALIAS=$(aws lambda list-aliases \
              --function-name ${FUNCTION_NAME} \
              --query 'Aliases[?Name==`previous`].FunctionVersion' \
              --output text \
              --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
            
            if [ -z "$PREVIOUS_ALIAS" ]; then
              # Try to get version from tags
              git fetch --tags --force || true
              PREVIOUS_TAG=$(git describe --tags --abbrev=0 HEAD~1 2>/dev/null || echo "")
              if [ -n "$PREVIOUS_TAG" ]; then
                PREVIOUS_VERSION=$(echo "$PREVIOUS_TAG" | sed 's/^v//')
                echo "version=${PREVIOUS_VERSION}" >> $GITHUB_OUTPUT
                echo "tag=${PREVIOUS_TAG}" >> $GITHUB_OUTPUT
                echo "Found previous version from tag: ${PREVIOUS_VERSION}"
              else
                echo "version=unknown" >> $GITHUB_OUTPUT
                echo "tag=unknown" >> $GITHUB_OUTPUT
                echo "Could not determine previous version"
              fi
            else
              echo "version=${PREVIOUS_ALIAS}" >> $GITHUB_OUTPUT
              echo "Found previous version from alias: ${PREVIOUS_ALIAS}"
            fi
          else
            echo "version=unknown" >> $GITHUB_OUTPUT
            echo "Could not determine previous version"
          fi

      - name: Rollback on failure
        if: steps.verify.outcome == 'failure' && steps.previous_version.outputs.version != 'unknown'
        env:
          PREVIOUS_VERSION: ${{ steps.previous_version.outputs.version }}
          PREVIOUS_TAG: ${{ steps.previous_version.outputs.tag }}
        run: |
          echo "âŒ Deployment verification failed, initiating rollback..."
          FUNCTION_NAME="meal-expense-tracker-dev"
          ECR_REPO="${{ steps.ecr_repo.outputs.repository }}"
          
          if [ -n "${PREVIOUS_TAG}" ] && [ "${PREVIOUS_TAG}" != "unknown" ]; then
            # Try to rollback to previous tagged version
            echo "Attempting to rollback to ${PREVIOUS_TAG}..."
            
            # Pull previous image from GHCR
            docker pull ghcr.io/${{ github.repository }}:${PREVIOUS_VERSION} || \
            docker pull ghcr.io/${{ github.repository }}:${PREVIOUS_TAG} || {
              echo "âš ï¸  Could not pull previous image, trying ECR..."
              # Try ECR
              docker pull ${ECR_REPO}:${PREVIOUS_VERSION} || {
                echo "âŒ Could not rollback - previous image not found"
                exit 1
              }
            }
            
            # Tag and push to ECR
            docker tag ghcr.io/${{ github.repository }}:${PREVIOUS_VERSION} ${ECR_REPO}:rollback || \
            docker tag ${ECR_REPO}:${PREVIOUS_VERSION} ${ECR_REPO}:rollback
            
            docker push ${ECR_REPO}:rollback || true
            
            # Update Lambda to rollback version
            aws lambda update-function-code \
              --function-name ${FUNCTION_NAME} \
              --image-uri ${ECR_REPO}:rollback \
              --region ${{ env.AWS_REGION }} || {
              echo "âŒ Failed to rollback Lambda function"
              exit 1
            }
            
            echo "âœ… Rolled back to version ${PREVIOUS_VERSION}"
          else
            echo "âš ï¸  Could not determine previous version for rollback"
            echo "Manual intervention required"
            exit 1
          fi

      - name: Verify rollback
        if: steps.verify.outcome == 'failure' && steps.previous_version.outputs.version != 'unknown'
        run: |
          DEPLOYMENT_URL="${{ steps.get_url.outputs.url }}"
          echo "Verifying rollback at: ${DEPLOYMENT_URL}"
          
          # Wait for Lambda to update
          sleep 30
          
          # Health check rollback
          for i in {1..5}; do
            if curl -f -s "${DEPLOYMENT_URL}/health" > /dev/null; then
              echo "âœ… Rollback verified - health check passed"
              exit 0
            fi
            echo "Rollback verification attempt ${i}/5 failed, retrying..."
            sleep 10
          done
          
          echo "âŒ Rollback verification failed"
          exit 1

      - name: Notify rollback
        if: steps.verify.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            const repo = context.repo;
            const workflowRunId = context.runId;
            const workflowUrl = `https://github.com/${repo.owner}/${repo.repo}/actions/runs/${workflowRunId}`;
            
            await github.rest.issues.create({
              ...repo,
              title: `ðŸš¨ Deployment Rollback Required - ${context.sha.slice(0, 7)}`,
              body: `## Deployment Verification Failed
              
              Deployment to **dev** environment failed health checks.
              
              - **Commit:** ${context.sha.slice(0, 7)}
              - **Workflow:** [View Run](${workflowUrl})
              - **Environment:** dev
              - **Status:** Rollback ${steps.rollback && steps.rollback.outcome === 'success' ? 'âœ… Successful' : 'âŒ Failed'}
              
              Manual intervention may be required.`,
              labels: ['deployment', 'rollback', 'urgent']
            });

  # Deploy to staging environment
  deploy-staging:
    name: Deploy to Staging
    needs: [check-ci, build]
    if: >
      ${{ github.event_name != 'pull_request' &&
      needs.check-ci.outputs.ci_passed == 'true' &&
      github.event.inputs.environment == 'staging' }}
    environment: staging
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-${{ github.run_id }}
          role-duration-seconds: 3600

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.5
          terraform_wrapper: false

      - name: Setup Terraform plugin cache
        run: |
          mkdir -p ${{ runner.temp }}/.terraform.d/plugin-cache
          echo "Terraform plugin cache directory created: ${{ runner.temp }}/.terraform.d/plugin-cache"

      - name: Generate backend.hcl files from templates
        env:
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          if [ -d "terraform" ]; then
            # Get AWS account ID
            AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            echo "Using AWS Account ID: $AWS_ACCOUNT_ID"
            echo "Using AWS Region: $AWS_REGION"
            # Generate backend.hcl files for each environment from templates
            for ENV in dev staging prod; do
              TEMPLATE_FILE="terraform/environments/${ENV}/backend.hcl.template"
              OUTPUT_FILE="terraform/environments/${ENV}/backend.hcl"
              if [ -f "$TEMPLATE_FILE" ]; then
                echo "Generating $OUTPUT_FILE from template..."
                # Replace ${AWS_ACCOUNT_ID} and ${AWS_REGION} placeholders
                sed -e "s/\${AWS_ACCOUNT_ID}/$AWS_ACCOUNT_ID/g" \
                    -e "s/\${AWS_REGION}/$AWS_REGION/g" \
                    "$TEMPLATE_FILE" > "$OUTPUT_FILE"
                echo "âœ… Generated $OUTPUT_FILE"
              else
                echo "âš ï¸  Template file $TEMPLATE_FILE not found, skipping..."
              fi
            done
          else
            echo "No terraform directory found, skipping backend.hcl generation"
          fi

      - name: Deploy to staging
        env:
          TF_PLUGIN_CACHE_DIR: ${{ runner.temp }}/.terraform.d/plugin-cache
          TF_IN_AUTOMATION: 1
        run: |
          make TF_ENV=staging tf-init
          make TF_ENV=staging tf-plan
          make TF_ENV=staging tf-apply

      - name: Verify Deployment
        run: |
          LOAD_BALANCER_DNS=$(aws cloudformation describe-stacks \
            --stack-name staging-stack \
            --query 'Stacks[0].Outputs[?OutputKey==`LoadBalancerDNS`].OutputValue' \
            --output text \
            --region ${{ env.AWS_REGION }})

          curl --retry 5 --retry-delay 5 --retry-max-time 30 $LOAD_BALANCER_DNS/health

  # Deploy to production environment
  deploy-prod:
    name: Deploy to Production
    needs: [check-ci, build]
    if: >
      ${{ github.event_name != 'pull_request' &&
      needs.check-ci.outputs.ci_passed == 'true' &&
      github.event.inputs.environment == 'prod' }}
    environment: prod
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-${{ github.run_id }}
          role-duration-seconds: 3600

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.5
          terraform_wrapper: false

      - name: Setup Terraform plugin cache
        run: |
          mkdir -p ${{ runner.temp }}/.terraform.d/plugin-cache
          echo "Terraform plugin cache directory created: ${{ runner.temp }}/.terraform.d/plugin-cache"

      - name: Generate backend.hcl files from templates
        env:
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          if [ -d "terraform" ]; then
            # Get AWS account ID
            AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            echo "Using AWS Account ID: $AWS_ACCOUNT_ID"
            echo "Using AWS Region: $AWS_REGION"
            # Generate backend.hcl files for each environment from templates
            for ENV in dev staging prod; do
              TEMPLATE_FILE="terraform/environments/${ENV}/backend.hcl.template"
              OUTPUT_FILE="terraform/environments/${ENV}/backend.hcl"
              if [ -f "$TEMPLATE_FILE" ]; then
                echo "Generating $OUTPUT_FILE from template..."
                # Replace ${AWS_ACCOUNT_ID} and ${AWS_REGION} placeholders
                sed -e "s/\${AWS_ACCOUNT_ID}/$AWS_ACCOUNT_ID/g" \
                    -e "s/\${AWS_REGION}/$AWS_REGION/g" \
                    "$TEMPLATE_FILE" > "$OUTPUT_FILE"
                echo "âœ… Generated $OUTPUT_FILE"
              else
                echo "âš ï¸  Template file $TEMPLATE_FILE not found, skipping..."
              fi
            done
          else
            echo "No terraform directory found, skipping backend.hcl generation"
          fi

      - name: Deploy to production
        env:
          TF_PLUGIN_CACHE_DIR: ${{ runner.temp }}/.terraform.d/plugin-cache
          TF_IN_AUTOMATION: 1
        run: |
          make TF_ENV=prod tf-init
          make TF_ENV=prod tf-plan
          make TF_ENV=prod tf-apply

      - name: Verify Deployment
        run: |
          LOAD_BALANCER_DNS=$(aws cloudformation describe-stacks \
            --stack-name prod-stack \
            --query 'Stacks[0].Outputs[?OutputKey==`LoadBalancerDNS`].OutputValue' \
            --output text \
            --region ${{ env.AWS_REGION }})

          curl --retry 5 --retry-delay 5 --retry-max-time 30 $LOAD_BALANCER_DNS/health


  # Deploy to staging after release is created (triggered by test workflow)
  deploy-staging-after-release:
    name: Deploy to Staging (After Release)
    if: >
      ${{ github.event_name == 'repository_dispatch' &&
      github.event.action == 'deploy-staging' }}
    runs-on: ubuntu-latest
    environment: staging
    permissions:
      id-token: write
      contents: read
      packages: read  # Need to read packages to pull from GHCR
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get latest version tag
        id: get_version
        run: |
          TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo "")
          VERSION=$(echo "$TAG" | sed 's/^v//')
          echo "tag=${TAG}" >> $GITHUB_OUTPUT
          echo "version=${VERSION}" >> $GITHUB_OUTPUT
          echo "Using tag: ${TAG}, version: ${VERSION}"

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-${{ github.run_id }}
          role-duration-seconds: 3600

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.5
          terraform_wrapper: false

      - name: Setup Terraform plugin cache
        run: |
          mkdir -p ${{ runner.temp }}/.terraform.d/plugin-cache
          echo "Terraform plugin cache directory created: ${{ runner.temp }}/.terraform.d/plugin-cache"

      - name: Generate backend.hcl files from templates
        env:
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          if [ -d "terraform" ]; then
            # Get AWS account ID
            AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            echo "Using AWS Account ID: $AWS_ACCOUNT_ID"
            echo "Using AWS Region: $AWS_REGION"
            # Generate backend.hcl files for each environment from templates
            for ENV in dev staging prod; do
              TEMPLATE_FILE="terraform/environments/${ENV}/backend.hcl.template"
              OUTPUT_FILE="terraform/environments/${ENV}/backend.hcl"
              if [ -f "$TEMPLATE_FILE" ]; then
                echo "Generating $OUTPUT_FILE from template..."
                # Replace ${AWS_ACCOUNT_ID} and ${AWS_REGION} placeholders
                sed -e "s/\${AWS_ACCOUNT_ID}/$AWS_ACCOUNT_ID/g" \
                    -e "s/\${AWS_REGION}/$AWS_REGION/g" \
                    "$TEMPLATE_FILE" > "$OUTPUT_FILE"
                echo "âœ… Generated $OUTPUT_FILE"
              else
                echo "âš ï¸  Template file $TEMPLATE_FILE not found, skipping..."
              fi
            done
          else
            echo "No terraform directory found, skipping backend.hcl generation"
          fi

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y make jq

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Deploy Infrastructure (Terraform)
        env:
          TF_PLUGIN_CACHE_DIR: ${{ runner.temp }}/.terraform.d/plugin-cache
          TF_IN_AUTOMATION: 1
        run: |
          make TF_ENV=staging tf-init
          make TF_ENV=staging tf-plan
          make TF_ENV=staging tf-apply

      - name: Get ECR repository URI
        id: ecr_repo
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_REPO="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/meal-expense-tracker-staging-lambda"
          echo "repository=${ECR_REPO}" >> $GITHUB_OUTPUT

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | \
            docker login --username AWS --password-stdin ${{ steps.ecr_repo.outputs.repository }}

      - name: Pull and push Docker image to ECR
        run: |
          VERSION="${{ steps.get_version.outputs.version }}"
          TAG="${{ steps.get_version.outputs.tag }}"
          ECR_REPO="${{ steps.ecr_repo.outputs.repository }}"
          
          # Determine which image tag to pull from GHCR
          # Try version tag first, then fall back to SHA if available
          if [ -n "${VERSION}" ] && [ "${VERSION}" != "dev" ]; then
            # Try pulling by version tag
            if docker pull ghcr.io/${{ github.repository }}:${VERSION} 2>/dev/null; then
              echo "Pulled image with version tag: ${VERSION}"
              docker tag ghcr.io/${{ github.repository }}:${VERSION} ${ECR_REPO}:${VERSION}
              docker tag ${ECR_REPO}:${VERSION} ${ECR_REPO}:latest
            # Try pulling by full tag (v0.6.0 format)
            elif [ -n "${TAG}" ] && docker pull ghcr.io/${{ github.repository }}:${TAG} 2>/dev/null; then
              echo "Pulled image with tag: ${TAG}"
              docker tag ghcr.io/${{ github.repository }}:${TAG} ${ECR_REPO}:${VERSION}
              docker tag ${ECR_REPO}:${VERSION} ${ECR_REPO}:latest
            # Fall back to SHA tag
            elif docker pull ghcr.io/${{ github.repository }}:${GITHUB_SHA::7} 2>/dev/null; then
              echo "Pulled image with SHA tag: ${GITHUB_SHA::7}"
              docker tag ghcr.io/${{ github.repository }}:${GITHUB_SHA::7} ${ECR_REPO}:${VERSION}
              docker tag ${ECR_REPO}:${VERSION} ${ECR_REPO}:latest
            else
              echo "Error: Could not pull image from GHCR with any available tag"
              echo "Tried: ${VERSION}, ${TAG}, ${GITHUB_SHA::7}"
              exit 1
            fi
          else
            echo "Error: No valid version or tag found for staging deployment"
            exit 1
          fi
          
          # Push both tags to ECR
          docker push ${ECR_REPO}:${VERSION}
          docker push ${ECR_REPO}:latest

      - name: Update Lambda function
        run: |
          FUNCTION_NAME="meal-expense-tracker-staging"
          ECR_REPO="${{ steps.ecr_repo.outputs.repository }}"
          
          aws lambda update-function-code \
            --function-name ${FUNCTION_NAME} \
            --image-uri ${ECR_REPO}:latest \
            --region ${{ env.AWS_REGION }}
          
          aws lambda wait function-updated \
            --function-name ${FUNCTION_NAME} \
            --region ${{ env.AWS_REGION }}

      - name: Sync static files to S3
        env:
          ENVIRONMENT: staging
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          chmod +x scripts/sync_static_to_s3.sh
          ./scripts/sync_static_to_s3.sh || echo "Warning: Static file sync failed"

      - name: Verify Deployment
        run: |
          API_URL=$(aws apigatewayv2 get-apis \
            --query "Items[?contains(Name, 'meal-expense-tracker-staging')].ApiEndpoint" \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "https://meals.staging.nivecher.com")
          
          for i in {1..10}; do
            if curl -f -s "${API_URL}/health" > /dev/null; then
              echo "âœ… Staging deployment verified"
              exit 0
            fi
            sleep 10
          done
          exit 1

  # Notify deployment status
  notify:
    name: Notify
    needs: [deploy-dev, deploy-staging, deploy-prod, check-ci]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Set environment status
        id: set_status
        run: |
          if [ "${{ job.status }}" = "success" ]; then
            echo "STATUS=succeeded" >> $GITHUB_OUTPUT
            echo "COLOR=good" >> $GITHUB_OUTPUT
            echo "EMOJI=âœ…" >> $GITHUB_OUTPUT
          else
            echo "STATUS=failed" >> $GITHUB_OUTPUT
            echo "COLOR=danger" >> $GITHUB_OUTPUT
            echo "EMOJI=âŒ" >> $GITHUB_OUTPUT
          fi
          echo "ENV=${{ github.event.inputs.environment || 'dev' }}" >> $GITHUB_OUTPUT

      - name: Set version and tag
        id: set_version
        # Use conditional expression with short-circuiting:
        # if result is 'success', access outputs; otherwise use 'unknown'
        # The && operator short-circuits, so outputs are only accessed
        # when result is 'success'
        env:
          VERSION_VALUE: ${{ needs.check-ci.outputs.version || 'unknown' }}
          TAG_VALUE: ${{ needs.check-ci.outputs.tag || 'unknown' }}
        run: |
          echo "VERSION=$VERSION_VALUE" >> $GITHUB_OUTPUT
          echo "TAG=$TAG_VALUE" >> $GITHUB_OUTPUT

      - name: Send Deployment Status
        uses: actions/github-script@v7
        if: success() || failure()
        env:
          VERSION: ${{ steps.set_version.outputs.VERSION || 'unknown' }}
          TAG: ${{ steps.set_version.outputs.TAG || 'unknown' }}
        with:
          script: |
            const { STATUS, COLOR, ENV, EMOJI, VERSION, TAG } = process.env;
            const repo = context.repo;
            const workflowRunId = context.runId;
            const workflowUrl = `https://github.com/${repo.owner}/${repo.repo}/actions/runs/${workflowRunId}`;

            // Create a comment on the PR if this is a PR
            if (context.eventName === 'pull_request') {
              await github.rest.issues.createComment({
                ...repo,
                issue_number: context.issue.number,
                body: `${EMOJI} **Deployment to ${ENV} ${STATUS}**\n\n` +
                      `- **Version:** ${VERSION}\n` +
                      `- **Tag:** ${TAG}\n` +
                      `- **Workflow:** [View Workflow Run](${workflowUrl})\n` +
                      `- **Commit:** ${context.sha.slice(0, 7)}\n` +
                      `- **Triggered by:** ${context.actor}\n`
              });
            }

            // Update PR check status
            await github.rest.checks.create({
              ...repo,
              name: `Deployment to ${ENV}`,
              head_sha: context.sha,
              status: 'completed',
              conclusion: STATUS === 'succeeded' ? 'success' : 'failure',
              details_url: workflowUrl,
              output: {
                title: `Deployment ${STATUS} - Version ${VERSION}`,
                summary: `Deployment to ${ENV} ${STATUS} - Version ${VERSION}`,
                text: `Workflow run: ${workflowUrl}\nVersion: ${VERSION}\nTag: ${TAG}`
              }
            });
